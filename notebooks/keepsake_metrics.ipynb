{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import fsum\n",
    "import keepsake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiments = keepsake.experiments.list()\n",
    "len(experiments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(experiments[-1].checkpoints[0])\n",
    "type(experiments[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiments[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = len(experiments[-1].checkpoints[:-1].metrics[\"ap\"])\n",
    "s = sum(experiments[-1].checkpoints[:-1].metrics[\"ap\"])\n",
    "print(steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recall, precision, F-1, avg. iteration duration, total duration\n",
    "def collect_aggregate_metrics(experiment: keepsake.experiment.Experiment):\n",
    "    aggregate_metrics = {}\n",
    "    aggregate_metrics[\"recall\"] = experiment.checkpoints[-1].metrics[\"calculated_metrics\"][\"ALL\"][\"r\"]\n",
    "    aggregate_metrics[\"precision\"] = experiment.checkpoints[-1].metrics[\"calculated_metrics\"][\"ALL\"][\"rels_found\"] / experiment.checkpoints[-1].metrics[\"calculated_metrics\"][\"ALL\"][\"last_rel\"]\n",
    "    aggregate_metrics[\"f-1\"] = 2 * ((aggregate_metrics[\"precision\"] * aggregate_metrics[\"recall\"]) / (aggregate_metrics[\"precision\"] + aggregate_metrics[\"recall\"]))\n",
    "    aggregate_metrics[\"total_duration\"] = experiment.checkpoints[-1].metrics[\"iteration_duration_seconds\"]\n",
    "    aggregate_metrics[\"avg_iteration_duration\"] = fsum(experiment.checkpoints[:-1].metrics[\"iteration_duration_seconds\"]) / (len(experiment.checkpoints) - 1)\n",
    "    return aggregate_metrics\n",
    "\n",
    "# stepwise recall, precision, duration\n",
    "def collect_stepwise_metrics(experiment: keepsake.experiment.Experiment):\n",
    "    stepwise_metrics = []\n",
    "    for checkpoint in experiment.checkpoints[:-1]:\n",
    "        step_metrics = {}\n",
    "        step_metrics[\"iteration\"] = checkpoint.metrics[\"iteration\"]\n",
    "        step_metrics[\"recall\"] = checkpoint.metrics[\"running_true_recall\"]\n",
    "        step_metrics[\"sampled_num\"] = checkpoint.metrics[\"sampled_num\"]\n",
    "        step_metrics[\"precision\"] = checkpoint.metrics[\"running_true_r\"] / checkpoint.metrics[\"sampled_num\"]\n",
    "        stepwise_metrics.append(step_metrics)\n",
    "    \n",
    "    return stepwise_metrics\n",
    "\n",
    "\n",
    "def collect_metrics_from_experiment(experiment: keepsake.experiment.Experiment):\n",
    "    aggregate_metrics = collect_metrics_from_experiment(experiment)\n",
    "    stepwise_metrics = collect_stepwise_metrics(experiment)\n",
    "    return {\"aggregate\": aggregate_metrics, \"stepwise\": stepwise_metrics}    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for experiment in experiments:\n",
    "    # baseline AutoTAR and experimental Fuzzy ARTMAP runs\n",
    "    # experiment.params[\"vectorizer_type\"]\n",
    "    if \"AutoTAR\" in experiment.params[\"run_group\"] or \"fam\" in experiment.params[\"run_group\"]:\n",
    "        metrics = collect_metrics_from_experiment(experiment)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# experiments[:-1].plot(\"running_true_recall\")\n",
    "# experiments.plot(\"running_true_recall\")\n",
    "exp_zero_data_x = []\n",
    "exp_zero_data_y = []\n",
    "for chk in experiments[0].checkpoints:\n",
    "    if chk.metrics[\"metric_type\"] == \"step\":\n",
    "        exp_zero_data_y.append(chk.metrics[\"running_true_recall\"])\n",
    "        exp_zero_data_x.append(chk.metrics[\"sampled_num\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_one_data_x = []\n",
    "exp_one_data_y = []\n",
    "for chk in experiments[1].checkpoints:\n",
    "    if chk.metrics[\"metric_type\"] == \"step\":\n",
    "        exp_one_data_y.append(chk.metrics[\"running_true_recall\"])\n",
    "        exp_one_data_x.append(chk.metrics[\"sampled_num\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(exp_zero_data_x, exp_zero_data_y, label=\"0\")\n",
    "plt.plot(exp_one_data_x, exp_one_data_y, label=\"1\")\n",
    "plt.xlabel(\"sample_num\")\n",
    "plt.ylabel(\"running_true_recall\")\n",
    "plt.legend(bbox_to_anchor=(1, 1))\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1c7fd97181157154b62cb21d4934004d93b01e25a27fd6eef6c2f2e3a46eb491"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
